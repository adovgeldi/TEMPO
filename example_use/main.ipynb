{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041a9b42-4201-49b3-a104-6932944876f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Instructions:\n",
    "\n",
    "- Run build_example_data.ipynb, which will populate rental_car_time_series.csv in the example_use directory\n",
    "- Run this notebook\n",
    "\n",
    "Notes:\n",
    "- This notebook is designed to be run locally, and as such, cannot utilize the parallel pipeline. \n",
    "  - Sample code to run the parallel pipeline has been inserted and commented out. \n",
    "  - In order to use the parallel pipeline, TEMPO requires PySpark and the ability to write to databricks tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68da3baf-3c0e-4c60-b14c-15a773ac0d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257f88cc-2024-43aa-9f53-827b842cd659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb5b021-3e81-4807-8d7b-5275fdf484f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from tempo_forecasting.utils.training_utils import calculate_time_periods\n",
    "\n",
    "from tempo_forecasting.utils.logging_utils import logger\n",
    "from tempo_forecasting.pipeline.parallel_pipeline import ParallelPipeline\n",
    "\n",
    "from tempo_forecasting.pipeline.preprocessing_pipeline import preprocess_pipeline\n",
    "from tempo_forecasting.pipeline.training_pipeline import train_pipeline\n",
    "from tempo_forecasting.pipeline.evaluation_pipeline import evaluation_pipeline\n",
    "\n",
    "from tempo_forecasting.utils.plotting_utils import plot_side_by_side\n",
    "from tempo_forecasting.optuna_opt.run_optuna import OptunaConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3cf5d41-8e5d-43dd-a8b2-edd9c2871bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b364a3-c116-4686-90de-4e92d0f710a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7865ca7-afe4-4dda-bfbf-72b622ecb991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demand_df = pd.read_csv(\"./rental_car_time_series.csv\")\n",
    "demand_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb306be-ec33-48c4-aa84-a7a6dadf0b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_date = demand_df[\"date\"].min()\n",
    "max_date = pd.Timestamp(demand_df[\"date\"].max())\n",
    "print(min_date,max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3973851c-c9a7-4c39-bedc-d2f06886a686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Arg Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557c51ed-0c21-4480-8469-b7b8e6f52f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_config = {\n",
    "    \"max_date\": str(max_date)[:10],\n",
    "    \"n_test_months\": 6,\n",
    "    \"n_train_months\": 36,\n",
    "    \"n_validation_sets\": 4,\n",
    "    \"cv_window_step_days\": 90\n",
    "}\n",
    "\n",
    "time_periods = calculate_time_periods(max_date_str = str(date_config['max_date']),\n",
    "                                      n_test_months = date_config[\"n_test_months\"],\n",
    "                                      n_train_months = date_config[\"n_train_months\"],\n",
    "                                      n_validation_sets = date_config[\"n_validation_sets\"],\n",
    "                                      cv_window_step_days = date_config[\"cv_window_step_days\"],\n",
    "                                      verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800b7df5-73f2-4cf5-8060-a4e6284ee240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'date_col': 'date',\n",
    "    'target_y': 'n_rented',\n",
    "    'freq': 'D',\n",
    "    'cv_dates': [[d[\"min\"],d[\"cutoff\"],d[\"max\"]] for d in time_periods[\"cv_windows\"]],\n",
    "    'retrain_dates': [time_periods[\"retrain_range\"][\"min\"],time_periods[\"retrain_range\"][\"max\"]],\n",
    "    'use_parallel': False,\n",
    "    'min_target_value': 5, # min target y value in time series to consider models outside moving average \n",
    "    'future_forecast_horizon': 365\n",
    "    }\n",
    "\n",
    "logger.info(f\"Initializing model args: {args}\")\n",
    "\n",
    "n_trials = 3\n",
    "optuna_config = OptunaConfig(\n",
    "    n_trials = n_trials,\n",
    "    n_startup_trials = 1,\n",
    "    timeout_sec = 15 * n_trials * date_config[\"n_validation_sets\"], # allocate 10 seconds per trial as program timeout\n",
    "    eval_metric = \"wmape\"\n",
    ")\n",
    "\n",
    "group_col = 'category'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c919ef88-5d73-4af8-ac73-752d921c44c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ff0149-3eef-41d9-a28f-321428ec1fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_init_model_charts(category,all_date_vals,param_df):\n",
    "    tmp_params = param_df[param_df[\"category\"]==category]\n",
    "\n",
    "    best_metric_type = tmp_params[\"metric_type\"][0]\n",
    "    best_metric_avg_val = round(tmp_params[\"cv_avg_metric\"][0],4)\n",
    "\n",
    "    p = tmp_params[\"best_params\"][0]\n",
    "\n",
    "    line_limit = 150\n",
    "    if len(p)>line_limit:\n",
    "        chunked_p = []\n",
    "        current_line = \"\"\n",
    "        for s in p.split(\",\"):\n",
    "            if len(current_line) + len(s) <= line_limit:\n",
    "                current_line += f\"{s}, \"\n",
    "            else:\n",
    "                chunked_p.append(f\"{current_line}\")\n",
    "                current_line = s\n",
    "        chunked_p.append(current_line)\n",
    "\n",
    "        p = \"\\n\".join(chunked_p)\n",
    "\n",
    "    cap_add = f\"; Cross-Validation Avg. {best_metric_type} = {best_metric_avg_val}\\n{p}\"\n",
    "\n",
    "    plot_side_by_side(category,all_date_vals,param_df,caption_add=cap_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9a978e-0ee4-433c-971d-970c2048de82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_base_colors():\n",
    "    # Create a color palette with enough colors for all model types\n",
    "    # Using a mix of qualitative colors that work well for both regular and vivid versions\n",
    "    base_colors = [\n",
    "        '#1f77b4',  # Blue\n",
    "        '#ff7f0e',  # Orange\n",
    "        '#2ca02c',  # Green\n",
    "        '#d62728',  # Red\n",
    "        '#9467bd',  # Purple\n",
    "        '#8c564b',  # Brown\n",
    "        '#e377c2',  # Pink\n",
    "        '#7f7f7f',  # Gray\n",
    "        '#bcbd22',  # Olive\n",
    "        '#17becf',  # Cyan\n",
    "        '#aec7e8',  # Light blue\n",
    "        '#ffbb78',  # Light orange\n",
    "        '#98df8a',  # Light green\n",
    "        '#ff9896',  # Light red\n",
    "        '#c5b0d5',  # Light purple\n",
    "    ]\n",
    "\n",
    "    return base_colors\n",
    "\n",
    "def plot_model_performance(df, \n",
    "                           figsize=(8, 6), \n",
    "                           alpha_regular=0.4, \n",
    "                           alpha_best=0.9, \n",
    "                           marker_size_regular=80, \n",
    "                           marker_size_best=50,\n",
    "                           logscale=False,\n",
    "                           title=\"Model Performance: MAE vs WMAPE\"):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of model performance metrics (MAE vs WMAPE).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with columns ['category', 'model_type', 'WMAPE', 'MAE', 'is_best_model']\n",
    "    figsize (tuple): Figure size as (width, height)\n",
    "    alpha_regular (float): Transparency for regular models (0-1)\n",
    "    alpha_best (float): Transparency for best models (0-1)\n",
    "    marker_size_regular (int): Size of regular model markers\n",
    "    marker_size_best (int): Size of best model markers\n",
    "    logscale (bool): whether to apply a logarithmic scale to the axes\n",
    "    title (str): Plot title\n",
    "    \n",
    "    Returns:\n",
    "    fig, ax: Matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input DataFrame\n",
    "    required_columns = ['category', 'model_type', 'WMAPE', 'MAE', 'is_best_model']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame cannot be empty\")\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Get unique model types\n",
    "    model_types = df['model_type'].unique()\n",
    "    \n",
    "    # Extend colors if we have more model types than base colors\n",
    "    base_colors = get_base_colors()\n",
    "    colors = base_colors * (len(model_types) // len(base_colors) + 1)\n",
    "    \n",
    "    # Create color mapping for model types\n",
    "    model_colors = {model_type: colors[i] for i, model_type in enumerate(model_types)}\n",
    "    \n",
    "    # Separate data into best models and regular models\n",
    "    best_models = df[df['is_best_model'] == True]\n",
    "    regular_models = df[df['is_best_model'] == False]\n",
    "    \n",
    "    # Plot regular models first (so best models appear on top)\n",
    "    for model_type in model_types:\n",
    "        regular_subset = regular_models[regular_models['model_type'] == model_type]\n",
    "        if not regular_subset.empty:\n",
    "            ax.scatter(regular_subset['WMAPE'], regular_subset['MAE'], \n",
    "                      color=model_colors[model_type], alpha=alpha_regular, \n",
    "                      s=marker_size_regular, label=f'{model_type} (regular)',\n",
    "                      edgecolors= None, linewidth=0.5)\n",
    "    \n",
    "    # Plot best models with vivid colors\n",
    "    for model_type in model_types:\n",
    "        best_subset = best_models[best_models['model_type'] == model_type]\n",
    "        if not best_subset.empty:\n",
    "            ax.scatter(best_subset['WMAPE'], best_subset['MAE'], \n",
    "                      color=model_colors[model_type], alpha=alpha_best, \n",
    "                      s=marker_size_best, label=f'{model_type} (best)',\n",
    "                      edgecolors=None, linewidth=1.5, marker='D')  # Diamond shape for best\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('WMAPE (Weighted Mean Absolute Percentage Error)', fontsize=12)\n",
    "    ax.set_ylabel('MAE (Mean Absolute Error)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if logscale:\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        ax.set_yscale('log')\n",
    "        ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "    \n",
    "    # Add legend\n",
    "    # Create a cleaner legend by grouping model types\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    # Create custom legend with better organization\n",
    "    legend_elements = []\n",
    "    for model_type in model_types:\n",
    "        # Add entry for regular models\n",
    "        if any(f'{model_type} (regular)' in label for label in labels):\n",
    "            legend_elements.append(plt.scatter([], [], color=model_colors[model_type], \n",
    "                                             s=marker_size_regular, alpha=alpha_regular, \n",
    "                                             edgecolors= None, linewidth=0.5,\n",
    "                                             label=f'{model_type}'))\n",
    "        # Add entry for best models\n",
    "        if any(f'{model_type} (best)' in label for label in labels):\n",
    "            legend_elements.append(plt.scatter([], [], color=model_colors[model_type], \n",
    "                                             s=marker_size_best, alpha=alpha_best, \n",
    "                                             edgecolors= None, linewidth=1.5, marker='D',\n",
    "                                             label=f'{model_type} (best)'))\n",
    "    \n",
    "    # Place legend outside the plot area\n",
    "    ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Add some styling improvements\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color('gray')\n",
    "    ax.spines['bottom'].set_color('gray')\n",
    "    \n",
    "    # Add text annotation with summary statistics\n",
    "    total_models = len(df)\n",
    "    best_models_count = len(best_models)\n",
    "    unique_categories = df['category'].nunique()\n",
    "    \n",
    "    stats_text = f'Total Models: {total_models}\\nCategories: {unique_categories}'\n",
    "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Adjust layout to prevent legend cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a20b329-fde1-43e3-a501-a1de9c2a8955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb23154d-3987-414c-ac86-b94a0ba6e22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if args['use_parallel']:\n",
    "    print(\"Parallel only supported in DataBricks via PySpark currently, sorry.\")\n",
    "    # run_timestamp = datetime.datetime.now(pytz.timezone('America/Chicago')).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # print(f\"JOB RUN_DATETIME: {run_timestamp}\")\n",
    "\n",
    "    # # group_sublist = [] #if you want to select a subset of categories to model\n",
    "\n",
    "    # # demand_df = modeling_df.where(F.col(\"category_group\").isin(group_sublist))\n",
    "\n",
    "    # parallel_pipeline = ParallelPipeline(run_id = run_timestamp,\n",
    "    #                                     group_col = \"category\",\n",
    "    #                                     args = args,\n",
    "    #                                     optuna_config = optuna_config,\n",
    "    #                                     target_metric = \"WMAPE\",\n",
    "    #                                     catalog_name=f\"dev_data_science\"\n",
    "    #                                     )\n",
    "\n",
    "    # modeling_results = parallel_pipeline.run_parallel_forecasting(demand_df)\n",
    "\n",
    "    # model_parameters_df, model_data_df, full_results_df = parallel_pipeline.postprocess_results(modeling_results)\n",
    "\n",
    "else:\n",
    "    results = {}\n",
    "    param_dfs = []\n",
    "    output_data_dfs = []\n",
    "\n",
    "    categories = demand_df[group_col].unique()\n",
    "\n",
    "    for category in categories:\n",
    "        # Filter the DataFrame for the current category\n",
    "        category_data = demand_df[demand_df[group_col]==category].drop(group_col, axis=1).copy()\n",
    "\n",
    "        # Step 1: Preprocessing\n",
    "        preproc_output = preprocess_pipeline(category = category,\n",
    "                                                category_data = category_data, \n",
    "                                                args = args,\n",
    "                                                logger = None)\n",
    "        model_set, cv_dates, _ = preproc_output\n",
    "        \n",
    "        # Step 2: Training\n",
    "        train_output = train_pipeline(category = category, \n",
    "                                        models = model_set, \n",
    "                                        modeling_data = category_data, \n",
    "                                        args = args, \n",
    "                                        optuna_config = optuna_config,\n",
    "                                        override_cv_dates = cv_dates,\n",
    "                                        logger = \"print\")   \n",
    "        \n",
    "        category_results, _ = train_output\n",
    "        results[category] = category_results\n",
    "\n",
    "        # # Step 3: Evaluation\n",
    "        eval_output = evaluation_pipeline(category = category,\n",
    "                                            category_results = category_results,\n",
    "                                            args = args,\n",
    "                                            target_metric = \"WMAPE\",\n",
    "                                            forecast_horizon = args['future_forecast_horizon'],\n",
    "                                            logger = \"print\")\n",
    "        category_param_df, category_output_data_df, _ = eval_output\n",
    "\n",
    "        param_dfs += [category_param_df]\n",
    "        output_data_dfs += [category_output_data_df]\n",
    "\n",
    "        # Build results json, slightly more complicated but same idea\n",
    "        results_cols = [\"category\",\"model\",\"cv_best_avg_metrics\",\n",
    "                        \"cv_best_all_metrics\",\"cv_all_trials_all_metrics\",\"best_params\"]\n",
    "        results_df = pd.DataFrame(columns=results_cols)\n",
    "\n",
    "        for cat in results:\n",
    "            for model in results[cat][\"models\"]:\n",
    "                cm_results = results[cat][\"models\"][model]\n",
    "\n",
    "                new_row = {'category': cat, \n",
    "                            'model': model,\n",
    "                            'cv_best_avg_metrics': cm_results[\"cv_metrics\"][\"cv_best_mean_all_metrics\"],\n",
    "                            'cv_best_all_metrics': cm_results[\"cv_metrics\"][\"cv_best_full_all_metrics\"],\n",
    "                            'cv_all_trials_all_metrics': cm_results[\"cv_metrics\"][\"cv_all_trials_all_metrics\"],\n",
    "                            'best_params': cm_results[\"model_params\"],\n",
    "                            }\n",
    "\n",
    "                results_df = pd.concat([results_df,pd.DataFrame([new_row])],ignore_index=True)\n",
    "\n",
    "        param_df = pd.concat(param_dfs)\n",
    "        output_data_df = pd.concat(output_data_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e637e28-0336-4d0d-ae89-43f7df0063c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Standard Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc40d80-975a-454d-927a-0aae7285d656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626ba446-e896-4879-8ecd-aa16645a9db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Details of the final model chosen by Optuna\n",
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3dc0900-8b06-428b-aa3d-a00818ed975e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In-depth details for all models run by optuna for each category. \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a799cc92-c4b1-40ef-8707-a0f88ff07d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final time series values. Contains the true values, the fit and pred values associated with the last cv window, and the fit and forecast values associated with the final retrained model\n",
    "output_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6d3ca3-76ec-440a-8c0d-69fbb272be20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## High Level Modeling Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957e52ea-1aeb-4969-b600-5cb688030f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optuna Trials per Model\n",
    "\n",
    "Three different things may determine when an Optuna study is ended:\n",
    "- Optuna has performed the intended number of trials, as specified in the optuna_config's n_trials parameter\n",
    "- Optuna has surpassed the intended time limit for a study, as specified in the optuna_config's timeout_sec parameter\n",
    "- The parameter search space was determined to be small enough for a grid search, and all possible parameter configurations were searched\n",
    "\n",
    "Each model type is prone to different types of study end conditions. As such, it is helpful to report statistics on how many studies were actually performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66afae09-b46b-4a1e-8057-f32adcc0356d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_trials_df = results_df[[\"category\", \"model\"]].copy()\n",
    "n_trials_df[\"n_trials\"] = results_df[\"cv_all_trials_all_metrics\"].apply(\n",
    "    lambda x: len(x[\"WMAPE\"]) if x is not None and \"WMAPE\" in x else 0\n",
    ")\n",
    "\n",
    "# Group by model and calculate aggregations\n",
    "result = n_trials_df.groupby(\"model\")[\"n_trials\"].agg([\n",
    "    (\"mean_n_trials\", \"mean\"),\n",
    "    (\"std_n_trials\", \"std\"),\n",
    "    (\"min_n_trials\", \"min\"),\n",
    "    (\"med_n_trials\", \"median\"),\n",
    "    (\"max_n_trials\", \"max\")\n",
    "]).round(2).T\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5d54dc-04ce-4a82-95bb-691794540ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MAE vs. WMAPE by model type\n",
    "\n",
    "WMAPE is generally preferred when comparing across differing scales of data, however, there is a tradeoff with MAE. \n",
    "\n",
    "For example, a model where only 10 cars are rented on average might show a high WMAPE (30%!), but MAE (3) would show that we forecasted 13 cars, which is more explainable.\n",
    "\n",
    "The following chart allows systematic assessment of this trade off, color coded by model type and whether the model was selected as \"best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e9b28b-3ce0-40a8-8d82-0019e41438d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_comparison_df = results_df[[\"category\"]]\n",
    "model_comparison_df[\"model_type\"] = results_df[\"model\"]\n",
    "cv_best_all_metrics = results_df[\"cv_best_all_metrics\"]\n",
    "\n",
    "model_comparison_df[\"WMAPE\"] = [round(np.mean(row[\"WMAPE\"]),3) for row in cv_best_all_metrics]\n",
    "model_comparison_df[\"MAE\"] = [round(np.mean(row[\"MAE\"]),3) for row in cv_best_all_metrics]\n",
    "\n",
    "selected_models = param_df[[\"category\"]]\n",
    "selected_models[\"model_type\"] = param_df[\"model_name\"]\n",
    "selected_models[\"is_best_model\"] = True\n",
    "\n",
    "model_comparison_df = model_comparison_df.merge(selected_models, on=[\"category\",\"model_type\"], how=\"left\").fillna({\"is_best_model\":False})\n",
    "model_comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0accdf4f-318f-4327-a007-638314f6c985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_model_performance(model_comparison_df, figsize=(9, 5), logscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b33389-4a5d-4336-9d30-251368a3b9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Single Category\n",
    "cat = \"Car A\"\n",
    "\n",
    "fig, ax = plot_model_performance(\n",
    "    model_comparison_df[model_comparison_df[\"category\"]==cat], \n",
    "    figsize=(9,5),\n",
    "    alpha_regular=0.5,\n",
    "    marker_size_best=60,\n",
    "    title=f\"{cat}: Model Performance Analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8c7f4d-b30f-414c-a368-9062dd890f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Individual Model Results\n",
    "side by side plots with additional parameter details in the caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a7f94f-ac06-4db3-9a74-87bcf21af7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    plot_init_model_charts(category = cat,\n",
    "                           all_date_vals = output_data_df,\n",
    "                           param_df = param_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562a833a-924b-4371-9c7a-2184828ba31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----------\n",
    "# Additional Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211f7a54-e3bf-426d-940f-32091c9f98a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Single Time Series Plots\n",
    "Same as above, but pulled out individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dde7faf-7b92-4500-a87b-8eb20837970a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tempo_forecasting.utils.plotting_utils import plot_time_series_demand, extract_series_from_date_vals\n",
    "category_of_interest = \"Car A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0f8c09-a534-4f00-af77-1fd2a175dc48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_series_dict = extract_series_from_date_vals(category_date_vals = output_data_df[(output_data_df[\"category\"] == category_of_interest)])\n",
    "\n",
    "# Train\n",
    "plot_time_series_demand(title = f\"Category: {category_of_interest}, Last CV Window\", \n",
    "                        x_label = \"Date\", \n",
    "                        y_label = \"Demand\",\n",
    "                        actual_dates = data_series_dict[\"actuals_full\"][\"date\"], \n",
    "                        actual_vals = data_series_dict[\"actuals_full\"][\"vals\"],\n",
    "                        fitted_train_dates = data_series_dict[\"fitted_train\"][\"date\"], \n",
    "                        fitted_train_vals = data_series_dict[\"fitted_train\"][\"vals\"],\n",
    "                        forecasted_test_dates = data_series_dict[\"forecasted_test\"][\"date\"], \n",
    "                        forecasted_test_vals = data_series_dict[\"forecasted_test\"][\"vals\"])\n",
    "\n",
    "# Forecast\n",
    "plot_time_series_demand(title = f\"Category: {category_of_interest}, Final Forecast\", \n",
    "                        x_label = \"Date\", \n",
    "                        y_label = \"Demand\",\n",
    "                        actual_dates = data_series_dict[\"actuals_full\"][\"date\"], \n",
    "                        actual_vals = data_series_dict[\"actuals_full\"][\"vals\"],\n",
    "                        fitted_train_dates = data_series_dict[\"refitted_train\"][\"date\"], \n",
    "                        fitted_train_vals = data_series_dict[\"refitted_train\"][\"vals\"],\n",
    "                        forecasted_dates = data_series_dict[\"forecasted\"][\"date\"], \n",
    "                        forecasted_vals = data_series_dict[\"forecasted\"][\"vals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1636fe0-f34c-453d-97c9-e87a2ec84057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deep Dive\n",
    "\n",
    "It can be helpful to visualize different steps of the modeling process instead of taking Optuna's word that it has selected the best model. In order to do so, we must re-run a slightly modified version of the modified pipeline to capture a higher level of detail. The pipeline passes out time series data only for the best model's final cv window and final re-fit forecast. The code below re-runs modeling and saves time series values for every CV window of every model type, using that model type's best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9867d55e-65cb-48e2-8cd8-7497b22104d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tempo_forecasting.utils.config_utils import get_models\n",
    "from tempo_forecasting.utils.training_utils import cross_validate\n",
    "from tempo_forecasting.utils.plotting_utils import core_plotter\n",
    "\n",
    "category_of_interest = \"Car A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b6f2c2-a75a-4624-9566-e6040970d0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract model details: models, associated classes, and associated parameters\n",
    "category_cond = (results_df[\"category\"] == category_of_interest)\n",
    "\n",
    "coi_model_details = results_df[category_cond][[\"model\",\"best_params\"]]\\\n",
    "                            .set_index(\"model\")\\\n",
    "                            .to_dict(orient=\"index\")\n",
    "\n",
    "model_classes = get_models(how=\"all\")\n",
    "for model_type in coi_model_details:\n",
    "    coi_model_details[model_type][\"model_class\"] = model_classes[model_type]\n",
    "\n",
    "# Extract modeling data\n",
    "coi_modeling_data = output_data_df[(output_data_df[\"category\"]==category_of_interest)&\n",
    "                                   (~output_data_df[\"true_vals\"].isna())][[\"date\",\"true_vals\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f24f787a-0907-4ed8-a5e2-656111e731cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "desired_metrics = [\"wmape\",\"mae\"]\n",
    "\n",
    "# Do CV once (basically, mini train pipeline)\n",
    "coi_all_results = {}\n",
    "for model_type in coi_model_details.keys():\n",
    "    cv_results = cross_validate(data = coi_modeling_data,      \n",
    "                                date_col = \"date\", \n",
    "                                target_col = \"true_vals\", \n",
    "                                model_class = coi_model_details[model_type][\"model_class\"],\n",
    "                                model_param_dict = coi_model_details[model_type][\"best_params\"], \n",
    "                                cv_dates = args[\"cv_dates\"], \n",
    "                                metrics = desired_metrics)\n",
    "\n",
    "    coi_all_results[model_type] = cv_results\n",
    "\n",
    "# Run Eval Pipeline\n",
    "eval_pipeline_output = {}\n",
    "for model_type in coi_all_results.keys():\n",
    "    data_dict = {\n",
    "        \"data_vals\": np.array(coi_modeling_data[\"true_vals\"]),\n",
    "        \"data_dates\": np.array(coi_modeling_data[\"date\"]),\n",
    "        \"train_test_split_dates\":\n",
    "            {\n",
    "                \"cv_dates\": \"date\",\n",
    "                \"simple_train_test_dates\": args[\"cv_dates\"][-1]\n",
    "            }\n",
    "    }\n",
    "\n",
    "    single_model_result = {\n",
    "        \"cv_metrics\": {\n",
    "            \"cv_best_mean_all_metrics\": {\n",
    "                \"WMAPE\": np.round(np.mean([coi_all_results[model_type][i][\"metrics\"][\"WMAPE\"] \n",
    "                                for i in range(len(coi_all_results[model_type]))]),2)\n",
    "            },\n",
    "            \"cv_best_full_all_metrics\": {\n",
    "                \"WMAPE\": [coi_all_results[model_type][i][\"metrics\"][\"WMAPE\"] \n",
    "                            for i in range(len(coi_all_results[model_type]))]\n",
    "            },\n",
    "        },\n",
    "        \"train_preds\": coi_all_results[model_type][-1][\"fitted_train_vals\"],\n",
    "        \"test_preds\": coi_all_results[model_type][-1][\"test_pred_vals\"],\n",
    "        \"model_params\": coi_model_details[model_type][\"best_params\"]\n",
    "    }\n",
    "\n",
    "    eval_pipeline_input = {\n",
    "        \"models\": {\n",
    "            model_type : single_model_result\n",
    "        },\n",
    "        \"data\": data_dict\n",
    "    }\n",
    "\n",
    "    final_param_df, vals_df, logger = evaluation_pipeline(category = category_of_interest,\n",
    "                                                        category_results = eval_pipeline_input,\n",
    "                                                        args = args,\n",
    "                                                        target_metric = \"WMAPE\",\n",
    "                                                        forecast_horizon = 365,\n",
    "                                                        logger = \"print\")\n",
    "    \n",
    "    eval_pipeline_output[model_type] = {\n",
    "        \"vals_df\" : vals_df,\n",
    "        \"param_df\" : final_param_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49686690-2c01-4e3c-b00c-f8ae0f2c95c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demonstration of Cross-Model Performance\n",
    "\n",
    "Don't judge the poorly performing models below too hard - they would be doing much better if this example notebook gave them more than three Optuna trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1545ee0b-4886-4357-acf4-c257315f891f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chart_type = \"train and pred\" \n",
    "# or: train only\n",
    "\n",
    "if chart_type == \"train and pred\":\n",
    "    for model_type in eval_pipeline_output:\n",
    "        plot_init_model_charts(category_of_interest,\n",
    "                               eval_pipeline_output[model_type][\"vals_df\"],\n",
    "                               eval_pipeline_output[model_type][\"param_df\"])\n",
    "\n",
    "if chart_type == \"train only\":\n",
    "    n_models = len(coi_all_results.keys())\n",
    "    fig, axes = plt.subplots(n_models,1, sharex=True)\n",
    "    fig.set_figheight(16)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    fig.suptitle(f\"{category_of_interest} - Model Comparison\")\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for model_type in coi_all_results.keys():\n",
    "        cv_min_date, cv_cutoff_date, cv_max_date = coi_all_results[model_type][-1][\"dates\"]\n",
    "        fitted_train_date_cond = (coi_modeling_data[\"date\"]>=cv_min_date)&(coi_modeling_data[\"date\"]<=cv_cutoff_date)\n",
    "        forecasted_test_date_cond = (coi_modeling_data[\"date\"]>cv_cutoff_date)&(coi_modeling_data[\"date\"]<=cv_max_date)\n",
    "\n",
    "        all_dates = pd.to_datetime(coi_modeling_data[\"date\"])\n",
    "\n",
    "        mean_model_wmape = np.round(np.mean([coi_all_results[model_type][i][\"metrics\"][\"WMAPE\"] \n",
    "                                                for i in range(len(coi_all_results[model_type]))]),2)\n",
    "        model_wmape = coi_all_results[model_type][-1][\"metrics\"][\"WMAPE\"]\n",
    "        core_plotter(title = f\"{model_type} model: final cv wmape = {model_wmape}, mean cv wmape = {mean_model_wmape}\",\n",
    "                                ax = axes[i],\n",
    "                                x_label = \"\",\n",
    "                                y_label = \"Rental Demand\",\n",
    "                                actual_dates = all_dates,\n",
    "                                actual_vals = coi_modeling_data[\"true_vals\"],\n",
    "                                fitted_train_dates = all_dates[fitted_train_date_cond],\n",
    "                                fitted_train_vals = coi_all_results[model_type][-1][\"fitted_train_vals\"],\n",
    "                                forecasted_test_dates = all_dates[forecasted_test_date_cond],\n",
    "                                forecasted_test_vals = coi_all_results[model_type][-1][\"test_pred_vals\"])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f2fe75-b178-4fc2-b0eb-9ca44ab0d171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demonstration of Walk Forward CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a113ff-b4c7-48d9-9a50-656c5fa5ea97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models_of_interest = [\"prophet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b158671-7186-43fe-ae02-2effd84481ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for model_type in models_of_interest:\n",
    "    n_cv_windows = len(coi_all_results[model_type])\n",
    "    fig, axes = plt.subplots(n_cv_windows,1, sharex=True)\n",
    "    fig.set_figheight(16)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "\n",
    "    mean_model_wmape = np.round(np.mean([coi_all_results[model_type][i][\"metrics\"][\"WMAPE\"] \n",
    "                                            for i in range(len(coi_all_results[model_type]))]),2)\n",
    "    fig.suptitle(f\"{category_of_interest} - {model_type} model - CV Window Comparison (mean wmape = {mean_model_wmape})\")\n",
    "\n",
    "\n",
    "    for window_i in range(n_cv_windows):\n",
    "        cv_min_date, cv_cutoff_date, cv_max_date = coi_all_results[model_type][window_i][\"dates\"]\n",
    "        fitted_train_date_cond = (coi_modeling_data[\"date\"]>=cv_min_date)&(coi_modeling_data[\"date\"]<=cv_cutoff_date)\n",
    "        forecasted_test_date_cond = (coi_modeling_data[\"date\"]>cv_cutoff_date)&(coi_modeling_data[\"date\"]<=cv_max_date)\n",
    "\n",
    "        all_dates = pd.to_datetime(coi_modeling_data[\"date\"])\n",
    "\n",
    "        window_wmape = coi_all_results[model_type][window_i][\"metrics\"][\"WMAPE\"]\n",
    "        core_plotter(title = f\"Window {window_i+1}: {cv_min_date} to {cv_max_date}, wmape = {window_wmape}\",\n",
    "                     ax = axes[window_i],\n",
    "                     x_label = \"\",\n",
    "                     y_label = \"Rental Demand\",\n",
    "                     actual_dates = all_dates,\n",
    "                     actual_vals = coi_modeling_data[\"true_vals\"],\n",
    "                     fitted_train_dates = all_dates[fitted_train_date_cond],\n",
    "                     fitted_train_vals = coi_all_results[model_type][window_i][\"fitted_train_vals\"],\n",
    "                     forecasted_test_dates = all_dates[forecasted_test_date_cond],\n",
    "                     forecasted_test_vals = coi_all_results[model_type][window_i][\"test_pred_vals\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1c00e6-a7cb-48ad-941f-bc950a8b4bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
